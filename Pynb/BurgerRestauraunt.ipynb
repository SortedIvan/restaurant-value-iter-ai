{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3> Training an AI agent to traverse an environment and collect materials in order using value iteration <h3>\n",
        "<h5> By Ivan Ovcharov & Veronika Valeva <h5>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Table of Contents\n",
        "\n",
        "* Introduction\n",
        "* Why value iteration?\n",
        "* Environment description\n",
        "* Python environment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Over the period of 9 weeks, we have been tasked to train an agent to learn over given constraints in a python environment. The project we chose to tackle is something that closely resembles the infamous <strong> frozen lake </strong> environment. With every environment, there are different ways of approaching how an agent's rules may be defined or what strategy may be used for it to <strong> \"learn\" </strong>.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After delving a bit deeper into what <strong> reinforcement learning </strong> really is, we made the decision that <strong> <i> Value Iteration </i> </strong> would be best suited for our environment and the given conditions/rules we have defined. But why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why value iteration?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For any given state, we first calculate the state-action values for all the possible <strong>actions</strong> from that given state. We then update the value function of that state with the greatest state-action value. The reason we decided not to utilize <i>policy iteration</i> instead, as we thought unnecessary to have calculations of the expected/mean state-action value. For an environment like ours, where no \"predictions\" must be made, value iteration was the best option at hand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With value iteration, we'd be able to terminate when the difference between all the new state values and the old state values is a relatively small value. Furthermore, for a grid-like environment, where all of the possible actions and <strong>\"reward\"</strong> positions are pre-defined, we'd be better off with iterating over all possible states."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ![Value iteration algorithm](images/value_iteration.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Environment description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start with defining what an <strong>environment</strong> really stands for. An environment in AI is what is surrounding the agent. The agent can take input from the environment and deliver output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As mentioned previously, the environment we have chosen to define is something that very closely resembles the <strong> frozen lake </strong> one by OpenGymAI. The rules are as follows:\n",
        "* The environment is a grid (initially a 5x5 but scaled down to a 2x2/2x3) where there are \"ingredients\" that the agent must collect.\n",
        "* The agent is only able to move up, down, left or right, depending on his position on the grid.\n",
        "* The agent may not go (for example) left, if left is outside of the grid bounds\n",
        "* The agent starts in a <i> starting state </i> and finishes in an <i> ending state</i>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once the agent collects all of the ingredients (must be done in the correct order, otherwise => agent restarts), the agent must \"leave\" by going to the end state. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The reward system that is utilized is based on getting items in correct order and finishing the game. There are also slight penalties: when agent steps on an empty cell, he loses a total of 0.2 points and when he steps on a cell, containing an ingredient => +1 points. The reason behind this is that the agent can not only learn how to collect the ingredients in the right order, but the -0.2 points serves as a bound that \"pushes\" the agent to finish the game in less moves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ![Value iteration algorithm](images/env.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Python environment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first start by defnining what kind of states there will be in our environment. In here, we define the empty state, E. Furthermore, we have one representing our imaginary lettuce and cheese, L and C respectively. Lastly, a state indicating the start and end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 434,
      "metadata": {},
      "outputs": [],
      "source": [
        "from enum import Enum\n",
        "import random\n",
        "from random import randint, choice\n",
        "from copy import copy\n",
        "\n",
        "E, L, C, START, END = ' ', 'L', 'C','START','END'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Action space\n",
        "We start by defining the action space for our world. The action space will look as follows:\n",
        "\n",
        "Actions:\n",
        "*  Up\n",
        "*  Down\n",
        "*  Left\n",
        "*  Right\n",
        "\n",
        "Actions `Up`, `Down`, `Left`, and `Right` all move the actor to a new position. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 435,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Inventory(Enum):\n",
        "    Empty = \"\"\n",
        "    C = \"C\"\n",
        "    L = \"L\"\n",
        "    CL = \"CL\"\n",
        "    LC = \"LC\"\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 436,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Action(Enum):\n",
        "    Up = 1\n",
        "    Down = 2\n",
        "    Left = 3\n",
        "    Right = 4\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then define the rule set of our environment. This is done within a python class that holds all of the methods needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 437,
      "metadata": {},
      "outputs": [],
      "source": [
        "class State():\n",
        "    def __init__(self, state,inventory, action, is_done):\n",
        "        self.state = state\n",
        "        self.inventory = inventory\n",
        "        self.action = action\n",
        "        self.is_done = is_done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 438,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RestaurantEnvironment():\n",
        "    def __init__(self, initial_state=None):\n",
        "        if initial_state is None:\n",
        "            self.__initial_state = [E for n in range(6)]\n",
        "            self.__initial_state[0] = START\n",
        "            self.__initial_state[2] = C \n",
        "            self.__initial_state[4] = L\n",
        "            self.__initial_state[5] = END\n",
        "            self.playerState = [0, \"\"]\n",
        "            self.reward = 0\n",
        "        else:\n",
        "            self.__initial_state = copy(initial_state)\n",
        "            self.__state = self.__initial_state\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        self.__state = self.__initial_state\n",
        "        return self.__state\n",
        "\n",
        "    # Based on playerposition (0 to 6), add a letter to the currentWord\n",
        "    def calculate_curr_word(self, playerPosition):\n",
        "        if playerPosition == 2 and self.playerState[1] == \"\":\n",
        "            return \"C\"\n",
        "        elif playerPosition == 4 and self.playerState[1] == \"C\":\n",
        "            return \"L\"\n",
        "        else:\n",
        "            return \"\"\n",
        "\n",
        "    def step(self, playerPosition):\n",
        "        if (self.calculate_curr_word(self.calculate_transition(playerPosition))) != \"\":\n",
        "            self.playerState[1] += self.calculate_curr_word(self.calculate_transition(playerPosition))\n",
        "            self.reward += 1\n",
        "        else:\n",
        "            self.reward -= 0.1\n",
        "        newPlayerPosition = self.calculate_transition(playerPosition)\n",
        "        self.playerState[0] = newPlayerPosition\n",
        "        observation = self.__state  # environment is fully observable\n",
        "        done = self.is_done()\n",
        "        if done:\n",
        "            self.reset()\n",
        "        return observation, done, self.reward, self.playerState[0], self.playerState[1]\n",
        "\n",
        "    def get_reward(self):\n",
        "        return self.reward;\n",
        "    \n",
        "    def get_reward_according_to_location(self, location):\n",
        "        if location == 2 or location == 4:\n",
        "            return 1\n",
        "        else:\n",
        "            return -0.1\n",
        "\n",
        "    def render(self):\n",
        "        BACKGROUND = [\n",
        "            ' S │   │ C │',\n",
        "            '───┼───┼───┼',\n",
        "            '   │ L │ E │',\n",
        "            '───┼───┼───┼',\n",
        "        ]\n",
        "        rendering = copy(BACKGROUND)\n",
        "        for n, S_n in enumerate(self.__state):\n",
        "            if S_n != E:\n",
        "                row = 2 * (n // 5)\n",
        "                col = 4 * (n % 5) + 1\n",
        "                line = rendering[row]\n",
        "                rendering[row] = line[:col] + S_n + line[col + 1:]\n",
        "\n",
        "        for line in rendering:\n",
        "            print(line)\n",
        "\n",
        "    # =========================================================\n",
        "    # public functions for agent to calculate optimal policy\n",
        "    # =========================================================\n",
        "\n",
        "    def calculate_transition(self, action: Action):\n",
        "        current_location = self.playerState[0]\n",
        "        next_location = None\n",
        "        if self.playerState[0] == 0:\n",
        "            if action == Action.Right:\n",
        "                next_location = current_location + 1\n",
        "            elif action == Action.Down:\n",
        "                next_location = current_location + 3\n",
        "        elif self.playerState[0] == 1:\n",
        "            if action == Action.Right:\n",
        "                next_location = current_location + 1\n",
        "            elif action == Action.Left:\n",
        "                next_location = current_location - 1\n",
        "            elif action == Action.Down:\n",
        "                next_location = current_location + 3\n",
        "        elif self.playerState[0] == 2:\n",
        "            if action == Action.Left:\n",
        "                next_location = current_location - 1\n",
        "            elif action == Action.Down:\n",
        "                next_location = current_location + 3\n",
        "        elif self.playerState[0] == 3:\n",
        "            if action == Action.Right:\n",
        "                next_location = current_location + 1\n",
        "            elif action == Action.Up:\n",
        "                next_location = current_location - 3\n",
        "        elif self.playerState[0] == 4:\n",
        "            if action == Action.Right:\n",
        "                next_location = current_location + 1\n",
        "            elif action == Action.Up:\n",
        "                next_location = current_location - 3\n",
        "            elif action == Action.Left:\n",
        "                next_location = current_location - 1\n",
        "        elif self.playerState[0] == 5:\n",
        "            if action == Action.Up:\n",
        "                next_location = current_location - 3\n",
        "            elif action == Action.Left:\n",
        "                next_location = current_location - 1\n",
        "\n",
        "        if next_location == None:\n",
        "            return current_location\n",
        "        else:\n",
        "            return next_location\n",
        "\n",
        "    def get_transition_probability(self, action: Action, new_inventory):\n",
        "        next_location = self.calculate_transition(action)\n",
        "        current_inventory = self.playerState[1]\n",
        "        if next_location == 2: # State 2 contains C\n",
        "            if new_inventory == current_inventory + \"C\":\n",
        "                    return 1\n",
        "            else:\n",
        "                    return 0\n",
        "        elif next_location == 4: # State 4 contains C\n",
        "            if new_inventory == current_inventory + \"L\":\n",
        "                    return 1\n",
        "            else:\n",
        "                    return 0\n",
        "        elif current_inventory == new_inventory:\n",
        "            return 1        \n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def get_transition_prob(self, action: Action, currentLocation):\n",
        "        next_location = self.calculate_transition(action)\n",
        "        current_location = currentLocation\n",
        "        if self.is_done(): # If the current state is an end state\n",
        "            return 0.0\n",
        "        if next_location == current_location:\n",
        "            return 0.0\n",
        "        else: \n",
        "            return 1\n",
        "\n",
        "    def is_done(self):\n",
        "        if self.playerState[1] == \"CL\" and self.playerState[0] == 5:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def is_done_givenState(self, state):\n",
        "        if self.playerState[1] == \"CL\" and state == 5:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "\n",
        "    def get_possible_states(self):\n",
        "        return [\n",
        "         State(0, \"\", Action.Down, False), State(0, \"\", Action.Right, False),\n",
        "         State(0, \"C\", Action.Down, False), State(0, \"C\", Action.Right, False),\n",
        "         State(0, \"CL\", Action.Down, False), State(0, \"CL\", Action.Right, False),\n",
        "         \n",
        "         State(1,\"\", Action.Left, False), State(1, \"\", Action.Right, False), State(1, \"\", Action.Down, False),\n",
        "         State(1,\"C\", Action.Left, False), State(1,\"C\", Action.Right, False), State(1,\"C\", Action.Down, False),\n",
        "         State(1,\"CL\", Action.Left, False), State(1,\"CL\", Action.Right, False), State(1,\"CL\", Action.Down, False),\n",
        "\n",
        "         State(2, \"\", Action.Left, False), State(2,\"\", Action.Down, False),\n",
        "         State(2, \"C\", Action.Left, False), State(2,\"C\", Action.Down, False),\n",
        "         State(2, \"CL\", Action.Left, False), State(2,\"CL\", Action.Down, True), \n",
        "\n",
        "         State(3,\"\", Action.Up, False), State(3, \"\", Action. Right, False),\n",
        "         State(3,\"C\", Action.Up, False), State(3, \"C\", Action.Right, False),\n",
        "         State(3,\"CL\", Action.Up, False), State(3, \"CL\", Action.Right, False),\n",
        "\n",
        "         State (4,\"\", Action.Up, False), State(4, \"\", Action.Right, False),\n",
        "         State (4,\"C\", Action.Up, False), State(4, \"C\", Action.Right, False),\n",
        "         State (4,\"CL\", Action.Up, False), State(4, \"CL\", Action.Right, True),\n",
        "         State(4, \"C\",Action.Left, False),  State(4, \"CL\",Action.Left, False),\n",
        "         State (5,\"C\", Action.Left, False),State(5,\"C\", Action.Up, False)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining the world\n",
        "After we have defined the environment, we are going to construct the world the actor will be acting in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 439,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_environment() -> RestaurantEnvironment:\n",
        "    environment = RestaurantEnvironment()\n",
        "    return environment\n",
        "\n",
        "restEnvironment = generate_environment()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 440,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " START │ C │ C │L\n",
            "───┼───┼───┼\n",
            " END │ L │ E │\n",
            "───┼───┼───┼\n"
          ]
        }
      ],
      "source": [
        "class Game():\n",
        "    # example of creation of an environment in the default state\n",
        "    mdp = RestaurantEnvironment()\n",
        "    mdp.reset()\n",
        "    mdp.render()\n",
        "    state = \"\"\n",
        "    reward = 0.0\n",
        "    done = False\n",
        "    playerPosition = 0\n",
        "    inventory = \"\"\n",
        "    \n",
        "    # state, done, reward = mdp.step(Action.Right)\n",
        "    # print(state, done, reward)\n",
        "    # state, done, reward = mdp.step(Action.Right)\n",
        "    # print(state, done, reward)\n",
        "    # state, done, reward = mdp.step(Action.Left)\n",
        "    # print(state, done, reward)\n",
        "    # state, done, reward = mdp.step(Action.Down)\n",
        "    # print(state, done, reward)\n",
        "    # state, done, reward = mdp.step(Action.Right)\n",
        "    # print(state, done, reward)\n",
        "    # print(mdp.playerState[0], mdp.playerState[1])\n",
        "    # print('possible (internal) game states:')\n",
        "\n",
        "\n",
        "game = Game()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Measuring performance\n",
        "In order to get an accurate idea of the performance of a function we define a set of helper functions which will run number of episodes with the given policy, and print some statistics such as the `mean` of the running time as well as the `standard deviation`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 441,
      "metadata": {},
      "outputs": [],
      "source": [
        "from statistics import mean, stdev\n",
        "\n",
        "def run_one_episode(policy, environment, max_iteration_timeout=1000):\n",
        "  environment.reset()\n",
        "  state = environment\n",
        "  total_reward = 0.0\n",
        "  done = False\n",
        "  nextState = 0\n",
        "  inventory = \"\"\n",
        "  \n",
        "  iteration = 0\n",
        "  while not done or iteration >= max_iteration_timeout:\n",
        "    next_action = policy(state)\n",
        "    state, done, reward, nextState, inventory = environment.step(next_action)\n",
        "    total_reward += reward\n",
        "    iteration += 1\n",
        "  return total_reward\n",
        "\n",
        "def measure_performance(policy, environment, nrof_episodes=100):\n",
        "  N = nrof_episodes\n",
        "  print(\"statistics over {} episodes\".format(N))\n",
        "  all_rewards = []\n",
        "  for _ in range(N):\n",
        "    episode_reward = run_one_episode(policy, environment)\n",
        "    all_rewards.append(episode_reward)\n",
        "  print(\"mean: {:6.2f}, sigma: {:6.2f}\".format(mean(all_rewards), stdev(all_rewards)))\n",
        "  print()\n",
        "  for n, episode_reward in enumerate(all_rewards[:5], 1):\n",
        "    print(\"ep: {:2d}, total reward: {:5.2f}\".format(n, episode_reward))\n",
        "  print(\".....\")\n",
        "  for n, episode_reward in enumerate(all_rewards[-5:], len(all_rewards) - 5):\n",
        "    print(\"ep: {:2d}, total reward: {:5.2f}\".format(n, episode_reward))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Random agent\n",
        "Here we test the performance of an agent who's policy is to pick a random action at each state. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 442,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "statistics over 100 episodes\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "<__main__.RestaurantEnvironment object at 0x0000027FBE8B23B0>\n",
            "mean: -163.16, sigma: 338.30\n",
            "\n",
            "ep:  1, total reward: -191.40\n",
            "ep:  2, total reward: -5.50\n",
            "ep:  3, total reward: -29.00\n",
            "ep:  4, total reward: -6.10\n",
            "ep:  5, total reward: -143.00\n",
            ".....\n",
            "ep: 95, total reward: -56.50\n",
            "ep: 96, total reward: -56.60\n",
            "ep: 97, total reward: -56.70\n",
            "ep: 98, total reward: -113.70\n",
            "ep: 99, total reward: -57.00\n"
          ]
        }
      ],
      "source": [
        "def policy_random(state: RestaurantEnvironment) -> Action:\n",
        "  action = random.choice([a for a in Action])\n",
        "  return action\n",
        "\n",
        "measure_performance(policy_random, restEnvironment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### VALUE ITERATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 443,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'state': 0, 'inventory': '', 'action': <Action.Down: 2>, 'is_done': False}\n",
            "{'state': 0, 'inventory': '', 'action': <Action.Right: 4>, 'is_done': False}\n",
            "{'state': 0, 'inventory': 'C', 'action': <Action.Down: 2>, 'is_done': False}\n",
            "{'state': 0, 'inventory': 'C', 'action': <Action.Right: 4>, 'is_done': False}\n",
            "{'state': 0, 'inventory': 'CL', 'action': <Action.Down: 2>, 'is_done': False}\n",
            "{'state': 0, 'inventory': 'CL', 'action': <Action.Right: 4>, 'is_done': False}\n",
            "{'state': 1, 'inventory': '', 'action': <Action.Left: 3>, 'is_done': False}\n",
            "{'state': 1, 'inventory': '', 'action': <Action.Right: 4>, 'is_done': False}\n",
            "{'state': 1, 'inventory': '', 'action': <Action.Down: 2>, 'is_done': False}\n",
            "{'state': 1, 'inventory': 'C', 'action': <Action.Left: 3>, 'is_done': False}\n",
            "{'state': 1, 'inventory': 'C', 'action': <Action.Right: 4>, 'is_done': False}\n",
            "{'state': 1, 'inventory': 'C', 'action': <Action.Down: 2>, 'is_done': False}\n",
            "{'state': 1, 'inventory': 'CL', 'action': <Action.Left: 3>, 'is_done': False}\n",
            "{'state': 1, 'inventory': 'CL', 'action': <Action.Right: 4>, 'is_done': False}\n",
            "{'state': 1, 'inventory': 'CL', 'action': <Action.Down: 2>, 'is_done': False}\n",
            "{'state': 2, 'inventory': '', 'action': <Action.Left: 3>, 'is_done': False}\n",
            "{'state': 2, 'inventory': '', 'action': <Action.Down: 2>, 'is_done': False}\n",
            "{'state': 2, 'inventory': 'C', 'action': <Action.Left: 3>, 'is_done': False}\n",
            "{'state': 2, 'inventory': 'C', 'action': <Action.Down: 2>, 'is_done': False}\n",
            "{'state': 2, 'inventory': 'CL', 'action': <Action.Left: 3>, 'is_done': False}\n",
            "{'state': 2, 'inventory': 'CL', 'action': <Action.Down: 2>, 'is_done': True}\n",
            "{'state': 3, 'inventory': '', 'action': <Action.Up: 1>, 'is_done': False}\n",
            "{'state': 3, 'inventory': '', 'action': <Action.Right: 4>, 'is_done': False}\n",
            "{'state': 3, 'inventory': 'C', 'action': <Action.Up: 1>, 'is_done': False}\n",
            "{'state': 3, 'inventory': 'C', 'action': <Action.Right: 4>, 'is_done': False}\n",
            "{'state': 3, 'inventory': 'CL', 'action': <Action.Up: 1>, 'is_done': False}\n",
            "{'state': 3, 'inventory': 'CL', 'action': <Action.Right: 4>, 'is_done': False}\n",
            "{'state': 4, 'inventory': '', 'action': <Action.Up: 1>, 'is_done': False}\n",
            "{'state': 4, 'inventory': '', 'action': <Action.Right: 4>, 'is_done': False}\n",
            "{'state': 4, 'inventory': 'C', 'action': <Action.Up: 1>, 'is_done': False}\n",
            "{'state': 4, 'inventory': 'C', 'action': <Action.Right: 4>, 'is_done': False}\n",
            "{'state': 4, 'inventory': 'CL', 'action': <Action.Up: 1>, 'is_done': False}\n",
            "{'state': 4, 'inventory': 'CL', 'action': <Action.Right: 4>, 'is_done': True}\n",
            "{'state': 4, 'inventory': 'C', 'action': <Action.Left: 3>, 'is_done': False}\n",
            "{'state': 4, 'inventory': 'CL', 'action': <Action.Left: 3>, 'is_done': False}\n",
            "{'state': 5, 'inventory': 'C', 'action': <Action.Left: 3>, 'is_done': False}\n",
            "{'state': 5, 'inventory': 'C', 'action': <Action.Up: 1>, 'is_done': False}\n",
            "Utilities:\n",
            "[ 0.0000] \n",
            "[ 0.0000] \n",
            "[ 0.0000] \n",
            "[ 0.0000] \n",
            "[ 0.0000] \n",
            "[ 0.0000] \n",
            "[ 0.0000] \n",
            "[ 0.0000] \n",
            "[ 0.0000] \n",
            "[ 0.0000] \n",
            "[ 0.0000] \n",
            "[ 0.0000] \n",
            "[ 0.0000] \n",
            "[ 0.0000] \n",
            "[ 0.0000] \n",
            "[ 0.0000] \n",
            "[ 0.0000] \n",
            "[ 0.0000] \n",
            "[ 0.0000] \n",
            "[ 0.0000] \n",
            "[ 0.0000] \n",
            "[ 0.0000] \n",
            "[ 0.0000] \n",
            "[ 0.0000] \n",
            "[ 0.0000] \n",
            "[ 0.0000] \n",
            "[ 0.0000] \n",
            "[ 0.0000] \n",
            "[ 0.0000] \n",
            "[ 0.0000] \n",
            "[ 0.0000] \n",
            "[ 0.0000] \n",
            "[ 0.0000] \n",
            "[ 0.0000] \n",
            "[ 0.0000] \n",
            "[ 0.0000] \n",
            "[ 0.0000] \n",
            "Up\n",
            "Up\n",
            "Up\n",
            "Up\n",
            "Up\n",
            "Up\n",
            "Up\n",
            "Up\n",
            "Up\n",
            "Up\n",
            "Up\n",
            "Up\n",
            "Up\n",
            "Up\n",
            "Up\n",
            "Up\n",
            "Up\n",
            "Up\n",
            "Up\n",
            "Up\n",
            "Up\n",
            "Up\n",
            "Up\n",
            "Up\n",
            "Up\n",
            "Up\n",
            "Up\n",
            "Up\n",
            "Up\n",
            "Up\n",
            "Up\n",
            "Up\n",
            "Up\n",
            "Up\n",
            "Up\n",
            "Policy:\n",
            "{'state': 0, 'inventory': '', 'action': <Action.Down: 2>, 'is_done': False}: 🡹 \n",
            "{'state': 0, 'inventory': '', 'action': <Action.Right: 4>, 'is_done': False}: 🡹 \n",
            "{'state': 0, 'inventory': 'C', 'action': <Action.Down: 2>, 'is_done': False}: 🡹 \n",
            "{'state': 0, 'inventory': 'C', 'action': <Action.Right: 4>, 'is_done': False}: 🡹 \n",
            "{'state': 0, 'inventory': 'CL', 'action': <Action.Down: 2>, 'is_done': False}: 🡹 \n",
            "{'state': 0, 'inventory': 'CL', 'action': <Action.Right: 4>, 'is_done': False}: 🡹 \n",
            "{'state': 1, 'inventory': '', 'action': <Action.Left: 3>, 'is_done': False}: 🡹 \n",
            "{'state': 1, 'inventory': '', 'action': <Action.Right: 4>, 'is_done': False}: 🡹 \n",
            "{'state': 1, 'inventory': '', 'action': <Action.Down: 2>, 'is_done': False}: 🡹 \n",
            "{'state': 1, 'inventory': 'C', 'action': <Action.Left: 3>, 'is_done': False}: 🡹 \n",
            "{'state': 1, 'inventory': 'C', 'action': <Action.Right: 4>, 'is_done': False}: 🡹 \n",
            "{'state': 1, 'inventory': 'C', 'action': <Action.Down: 2>, 'is_done': False}: 🡹 \n",
            "{'state': 1, 'inventory': 'CL', 'action': <Action.Left: 3>, 'is_done': False}: 🡹 \n",
            "{'state': 1, 'inventory': 'CL', 'action': <Action.Right: 4>, 'is_done': False}: 🡹 \n",
            "{'state': 1, 'inventory': 'CL', 'action': <Action.Down: 2>, 'is_done': False}: 🡹 \n",
            "{'state': 2, 'inventory': '', 'action': <Action.Left: 3>, 'is_done': False}: 🡹 \n",
            "{'state': 2, 'inventory': '', 'action': <Action.Down: 2>, 'is_done': False}: 🡹 \n",
            "{'state': 2, 'inventory': 'C', 'action': <Action.Left: 3>, 'is_done': False}: 🡹 \n",
            "{'state': 2, 'inventory': 'C', 'action': <Action.Down: 2>, 'is_done': False}: 🡹 \n",
            "{'state': 2, 'inventory': 'CL', 'action': <Action.Left: 3>, 'is_done': False}: 🡹 \n",
            "           \n",
            "{'state': 3, 'inventory': '', 'action': <Action.Up: 1>, 'is_done': False}: 🡹 \n",
            "{'state': 3, 'inventory': '', 'action': <Action.Right: 4>, 'is_done': False}: 🡹 \n",
            "{'state': 3, 'inventory': 'C', 'action': <Action.Up: 1>, 'is_done': False}: 🡹 \n",
            "{'state': 3, 'inventory': 'C', 'action': <Action.Right: 4>, 'is_done': False}: 🡹 \n",
            "{'state': 3, 'inventory': 'CL', 'action': <Action.Up: 1>, 'is_done': False}: 🡹 \n",
            "{'state': 3, 'inventory': 'CL', 'action': <Action.Right: 4>, 'is_done': False}: 🡹 \n",
            "{'state': 4, 'inventory': '', 'action': <Action.Up: 1>, 'is_done': False}: 🡹 \n",
            "{'state': 4, 'inventory': '', 'action': <Action.Right: 4>, 'is_done': False}: 🡹 \n",
            "{'state': 4, 'inventory': 'C', 'action': <Action.Up: 1>, 'is_done': False}: 🡹 \n",
            "{'state': 4, 'inventory': 'C', 'action': <Action.Right: 4>, 'is_done': False}: 🡹 \n",
            "{'state': 4, 'inventory': 'CL', 'action': <Action.Up: 1>, 'is_done': False}: 🡹 \n",
            "           \n",
            "{'state': 4, 'inventory': 'C', 'action': <Action.Left: 3>, 'is_done': False}: 🡹 \n",
            "{'state': 4, 'inventory': 'CL', 'action': <Action.Left: 3>, 'is_done': False}: 🡹 \n",
            "{'state': 5, 'inventory': 'C', 'action': <Action.Left: 3>, 'is_done': False}: 🡹 \n",
            "{'state': 5, 'inventory': 'C', 'action': <Action.Up: 1>, 'is_done': False}: 🡹 \n"
          ]
        }
      ],
      "source": [
        "# Optimal decisions based on sums of rewards\n",
        "# state, done, reward, nextState, inventory = environment.step(next_action)\n",
        "# Function that computes the Q-value\n",
        "def Q_Value(restEnvironment, a, U, inventory):\n",
        "    Q = 0.0\n",
        "    for s in restEnvironment.get_possible_states():\n",
        "        P = restEnvironment.get_transition_probability(a, inventory)\n",
        "        R = restEnvironment.get_reward_according_to_location(getattr(s, 'state'))\n",
        "        Q += P * (R + U[str(s.__dict__)])\n",
        "    return Q\n",
        "\n",
        "# Creating a grid with initial values for our utility functions equal the reward of each state\n",
        "def get_initial_U(environment):\n",
        "  U = {}\n",
        "  for s in environment.get_possible_states():\n",
        "    print(s.__dict__)\n",
        "    U[str(s.__dict__)] = 0.0\n",
        "  \n",
        "  return U\n",
        "\n",
        "# Decreasing the error between utility functions results in a more precise policy \n",
        "def value_iteration(restEnvironment, error=0.0000000001):\n",
        "    U = {}\n",
        "    U_p = get_initial_U(restEnvironment)\n",
        "    delta = float('inf')\n",
        "    possible_states = restEnvironment.get_possible_states()\n",
        "    while delta > error:\n",
        "        for s in possible_states:\n",
        "            U[str(s.__dict__)] = U_p[str(s.__dict__)]\n",
        "        delta = 0\n",
        "        for s in possible_states:\n",
        "            max_a = float('-inf')\n",
        "            for a in Action:\n",
        "                for i in Inventory:\n",
        "                    q = Q_Value(restEnvironment, a, U, i) \n",
        "                    if q > max_a:\n",
        "                        max_a = q\n",
        "            U_p[str(s.__dict__)] = max_a\n",
        "            if abs(U_p[str(s.__dict__)] - U[str(s.__dict__)]) > delta:\n",
        "                delta = abs(U_p[str(s.__dict__)] - U[str(s.__dict__)])\n",
        "    return U\n",
        "\n",
        "def print_U(U):\n",
        "    print('Utilities:')\n",
        "    for y in restEnvironment.get_possible_states():\n",
        "        s = str(y.__dict__)\n",
        "        if s in U:\n",
        "            print(f'[{U[s]:7.4f}]', end = ' ')\n",
        "        else:\n",
        "            print('              ', end = '') # Used to preserve alignment when state is not present\n",
        "        print()\n",
        "\n",
        "def print_policy(pi):\n",
        "    arrows_unicode = {Action.Up: u\"\\U0001F879\", Action.Down: u\"\\U0001F87B\", Action.Left: u\"\\U0001F878\", Action.Right: u\"\\U0001F87A\"}\n",
        "    print('Policy:')\n",
        "    for y in restEnvironment.get_possible_states():\n",
        "        s = str(y.__dict__)\n",
        "        if s in pi:\n",
        "            print(f'{s}: {arrows_unicode[pi[s]]}', end = ' ')\n",
        "        else:\n",
        "            print(' '*11, end = '') # Used to preserve alignment when state is not present\n",
        "        print()\n",
        "\n",
        "restEnvironment = generate_environment()\n",
        "restEnvironment.reset()\n",
        "U = value_iteration(restEnvironment)\n",
        "print_U(U)\n",
        "\n",
        "# Optimal policy (p*)\n",
        "pi_star = {}\n",
        "\n",
        "for s in restEnvironment.get_possible_states():\n",
        "    if s.is_done:\n",
        "      continue\n",
        "    max_a = float('-inf')\n",
        "    argmax_a = None\n",
        "    for i in Inventory:\n",
        "        for action in Action:\n",
        "            q = Q_Value(restEnvironment, action, U, i)  \n",
        "            if q > max_a:\n",
        "                max_a = q\n",
        "                argmax_a = action\n",
        "    pi_star[str(s.__dict__)] = argmax_a\n",
        "    print(pi_star[str(s.__dict__)])\n",
        "\n",
        "print_policy(pi_star)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 444,
      "metadata": {},
      "outputs": [],
      "source": [
        "from statistics import mean, stdev\n",
        "\n",
        "def run_one_episode_optimal_policy(policy, environment, max_iteration_timeout=1000):\n",
        "  environment.reset()\n",
        "  state = environment\n",
        "  total_reward = 0.0\n",
        "  done = False\n",
        "  nextState = 0\n",
        "  inventory = \"\"\n",
        "  \n",
        "  iteration = 0\n",
        "  while not done or iteration >= max_iteration_timeout:\n",
        "    next_action = policy(state)\n",
        "    state, done, reward, nextState, inventory = environment.step(next_action)\n",
        "    total_reward += reward\n",
        "    iteration += 1\n",
        "  return total_reward\n",
        "\n",
        "def measure_performance_optimal_policy(policy, environment, nrof_episodes=100):\n",
        "  N = nrof_episodes\n",
        "  print(\"statistics over {} episodes\".format(N))\n",
        "  all_rewards = []\n",
        "  for _ in range(N):\n",
        "    episode_reward = run_one_episode_optimal_policy(policy, environment)\n",
        "    all_rewards.append(episode_reward)\n",
        "  print(\"mean: {:6.2f}, sigma: {:6.2f}\".format(mean(all_rewards), stdev(all_rewards)))\n",
        "  print()\n",
        "  for n, episode_reward in enumerate(all_rewards[:5], 1):\n",
        "    print(\"ep: {:2d}, total reward: {:5.2f}\".format(n, episode_reward))\n",
        "  print(\".....\")\n",
        "  for n, episode_reward in enumerate(all_rewards[-5:], len(all_rewards) - 5):\n",
        "    print(\"ep: {:2d}, total reward: {:5.2f}\".format(n, episode_reward))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 445,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "statistics over 100 episodes\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "optimal_policy() missing 1 required positional argument: 'state'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Lenovo\\Desktop\\BurgerRestauraunt.ipynb Cell 35'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Lenovo/Desktop/BurgerRestauraunt.ipynb#ch0000033?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(pi_star[state])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Lenovo/Desktop/BurgerRestauraunt.ipynb#ch0000033?line=2'>3</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m pi_star[state]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Lenovo/Desktop/BurgerRestauraunt.ipynb#ch0000033?line=4'>5</a>\u001b[0m measure_performance_optimal_policy(optimal_policy, restEnvironment)\n",
            "\u001b[1;32mc:\\Users\\Lenovo\\Desktop\\BurgerRestauraunt.ipynb Cell 34'\u001b[0m in \u001b[0;36mmeasure_performance_optimal_policy\u001b[1;34m(policy, environment, nrof_episodes)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lenovo/Desktop/BurgerRestauraunt.ipynb#ch0000034?line=21'>22</a>\u001b[0m all_rewards \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lenovo/Desktop/BurgerRestauraunt.ipynb#ch0000034?line=22'>23</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(N):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Lenovo/Desktop/BurgerRestauraunt.ipynb#ch0000034?line=23'>24</a>\u001b[0m   episode_reward \u001b[39m=\u001b[39m run_one_episode_optimal_policy(policy, environment)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lenovo/Desktop/BurgerRestauraunt.ipynb#ch0000034?line=24'>25</a>\u001b[0m   all_rewards\u001b[39m.\u001b[39mappend(episode_reward)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lenovo/Desktop/BurgerRestauraunt.ipynb#ch0000034?line=25'>26</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmean: \u001b[39m\u001b[39m{:6.2f}\u001b[39;00m\u001b[39m, sigma: \u001b[39m\u001b[39m{:6.2f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(mean(all_rewards), stdev(all_rewards)))\n",
            "\u001b[1;32mc:\\Users\\Lenovo\\Desktop\\BurgerRestauraunt.ipynb Cell 34'\u001b[0m in \u001b[0;36mrun_one_episode_optimal_policy\u001b[1;34m(policy, environment, max_iteration_timeout)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lenovo/Desktop/BurgerRestauraunt.ipynb#ch0000034?line=10'>11</a>\u001b[0m iteration \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lenovo/Desktop/BurgerRestauraunt.ipynb#ch0000034?line=11'>12</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done \u001b[39mor\u001b[39;00m iteration \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m max_iteration_timeout:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Lenovo/Desktop/BurgerRestauraunt.ipynb#ch0000034?line=12'>13</a>\u001b[0m   next_action \u001b[39m=\u001b[39m policy(state)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lenovo/Desktop/BurgerRestauraunt.ipynb#ch0000034?line=13'>14</a>\u001b[0m   state, done, reward, nextState, inventory \u001b[39m=\u001b[39m environment\u001b[39m.\u001b[39mstep(next_action)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lenovo/Desktop/BurgerRestauraunt.ipynb#ch0000034?line=14'>15</a>\u001b[0m   total_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n",
            "\u001b[1;31mTypeError\u001b[0m: optimal_policy() missing 1 required positional argument: 'state'"
          ]
        }
      ],
      "source": [
        "def optimal_policy(state) -> Action:\n",
        "    print(pi_star[state])\n",
        "    return pi_star[state]\n",
        "\n",
        "measure_performance_optimal_policy(optimal_policy, restEnvironment)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "BurgerRestauraunt.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "8c3d4d9970fc5c09c50d9318a9c7fbf4d0159e4b28833276f5678ee22bded273"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
