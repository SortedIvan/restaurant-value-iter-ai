{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3> Training an AI agent to traverse an environment and collect materials in order using value iteration <h3>\n",
        "<h5> By Ivan Ovcharov & Veronika Valeva <h5>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Table of Contents\n",
        "\n",
        "* Introduction\n",
        "* Why value iteration?\n",
        "* Environment description\n",
        "* Python environment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Over the period of 9 weeks, we have been tasked to train an agent to learn over given constraints in a python environment. The project we chose to tackle is something that closely resembles the infamous <strong> frozen lake </strong> environment. With every environment, there are different ways of approaching how an agent's rules may be defined or what strategy may be used for it to <strong> \"learn\" </strong>.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After delving a bit deeper into what <strong> reinforcement learning </strong> really is, we made the decision that <strong> <i> Value Iteration </i> </strong> would be best suited for our environment and the given conditions/rules we have defined. But why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why value iteration?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For any given state, we first calculate the state-action values for all the possible <strong>actions</strong> from that given state. We then update the value function of that state with the greatest state-action value. The reason we decided not to utilize <i>policy iteration</i> instead, as we thought unnecessary to have calculations of the expected/mean state-action value. For an environment like ours, where no \"predictions\" must be made, value iteration was the best option at hand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With value iteration, we'd be able to terminate when the difference between all the new state values and the old state values is a relatively small value. Furthermore, for a grid-like environment, where all of the possible actions and <strong>\"reward\"</strong> positions are pre-defined, we'd be better off with iterating over all possible states."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ![Value iteration algorithm](images/value_iteration.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Environment description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start with defining what an <strong>environment</strong> really stands for. An environment in AI is what is surrounding the agent. The agent can take input from the environment and deliver output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As mentioned previously, the environment we have chosen to define is something that very closely resembles the <strong> frozen lake </strong> one by OpenGymAI. The rules are as follows:\n",
        "* The environment is a grid (initially a 5x5 but scaled down to a 2x2/2x3) where there are \"ingredients\" that the agent must collect.\n",
        "* The agent is only able to move up, down, left or right, depending on his position on the grid.\n",
        "* The agent may not go (for example) left, if left is outside of the grid bounds\n",
        "* The agent starts in a <i> starting state </i> and finishes in an <i> ending state</i>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once the agent collects all of the ingredients (must be done in the correct order, otherwise => agent restarts), the agent must \"leave\" by going to the end state. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The reward system that is utilized is based on getting items in correct order and finishing the game. There are also slight penalties: when agent steps on an empty cell, he loses a total of 0.2 points and when he steps on a cell, containing an ingredient => +1 points. The reason behind this is that the agent can not only learn how to collect the ingredients in the right order, but the -0.2 points serves as a bound that \"pushes\" the agent to finish the game in less moves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ![Environment image](images/env.png)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "BurgerRestauraunt.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "1c1736cc465fff578baf12fdb47ac8eb299976fae09570724a499b71de6bfef0"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
