{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3> Training an AI agent to traverse an environment and collect materials in order using value iteration <h3>\n",
        "<h5> By Ivan Ovcharov & Veronika Valeva <h5>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Table of Contents\n",
        "\n",
        "* Introduction\n",
        "* Why value iteration?\n",
        "* Environment description\n",
        "* Python environment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Over the period of 9 weeks, we have been tasked to train an agent to learn over given constraints in a python environment. The project we chose to tackle is something that closely resembles the infamous <strong> frozen lake </strong> environment. With every environment, there are different ways of approaching how an agent's rules may be defined or what strategy may be used for it to <strong> \"learn\" </strong>.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After delving a bit deeper into what <strong> reinforcement learning </strong> really is, we made the decision that <strong> <i> Value Iteration </i> </strong> would be best suited for our environment and the given conditions/rules we have defined. But why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why value iteration?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For any given state, we first calculate the state-action values for all the possible <strong>actions</strong> from that given state. We then update the value function of that state with the greatest state-action value. The reason we decided not to utilize <i>policy iteration</i> instead, as we thought unnecessary to have calculations of the expected/mean state-action value. For an environment like ours, where no \"predictions\" must be made, value iteration was the best option at hand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With value iteration, we'd be able to terminate when the difference between all the new state values and the old state values is a relatively small value. Furthermore, for a grid-like environment, where all of the possible actions and <strong>\"reward\"</strong> positions are pre-defined, we'd be better off with iterating over all possible states."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ![Value iteration algorithm](images/value_iteration.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Environment description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start with defining what an <strong>environment</strong> really stands for. An environment in AI is what is surrounding the agent. The agent can take input from the environment and deliver output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As mentioned previously, the environment we have chosen to define is something that very closely resembles the <strong> frozen lake </strong> one by OpenGymAI. The rules are as follows:\n",
        "* The environment is a grid (initially a 5x5 but scaled down to a 2x2/2x3) where there are \"ingredients\" that the agent must collect.\n",
        "* The agent is only able to move up, down, left or right, depending on his position on the grid.\n",
        "* The agent may not go (for example) left, if left is outside of the grid bounds\n",
        "* The agent starts in a <i> starting state </i> and finishes in an <i> ending state</i>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once the agent collects all of the ingredients (must be done in the correct order, otherwise => agent restarts), the agent must \"leave\" by going to the end state. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The reward system that is utilized is based on getting items in correct order and finishing the game. There are also slight penalties: when agent steps on an empty cell, he loses a total of 0.2 points and when he steps on a cell, containing an ingredient => +1 points. The reason behind this is that the agent can not only learn how to collect the ingredients in the right order, but the -0.2 points serves as a bound that \"pushes\" the agent to finish the game in less moves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ![Value iteration algorithm](images/env.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Python environment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first start by defnining what kind of states there will be in our environment. In here, we define the empty state, E. Furthermore, we have one representing our imaginary lettuce and cheese, L and C respectively. Lastly, a state indicating the start and end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "from enum import Enum\n",
        "import random\n",
        "from random import randint, choice\n",
        "from copy import copy\n",
        "\n",
        "E, L, C, START, END = ' ', 'L', 'C','START','END'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Action space\n",
        "We start by defining the action space for our world. The action space will look as follows:\n",
        "\n",
        "Actions:\n",
        "*  Up\n",
        "*  Down\n",
        "*  Left\n",
        "*  Right\n",
        "\n",
        "Actions `Up`, `Down`, `Left`, and `Right` all move the actor to a new position. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Action(Enum):\n",
        "    Up = 1\n",
        "    Down = 2\n",
        "    Left = 3\n",
        "    Right = 4\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then define the rule set of our environment. This is done within a python class that holds all of the methods needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RestaurantEnvironment():\n",
        "    def __init__(self, initial_state=None):\n",
        "        if initial_state is None:\n",
        "            self.__initial_state = [E for n in range(6)]\n",
        "            self.__initial_state[0] = START\n",
        "            self.__initial_state[2] = C \n",
        "            self.__initial_state[4] = L\n",
        "            self.__initial_state[5] = END\n",
        "            self.playerState = [0, \"\"]\n",
        "            self.reward = 0\n",
        "        else:\n",
        "            self.__initial_state = copy(initial_state)\n",
        "            self.__state = self.__initial_state\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        self.__state = self.__initial_state\n",
        "        return self.__state\n",
        "\n",
        "    # Based on playerposition (0 to 6), add a letter to the currentWord\n",
        "    def calculate_curr_word(self, playerPosition):\n",
        "        if playerPosition == 2 and self.playerState[1] == \"\":\n",
        "            return \"C\"\n",
        "        elif playerPosition == 4 and self.playerState[1] == \"C\":\n",
        "            return \"L\"\n",
        "        else:\n",
        "            return \"\"\n",
        "\n",
        "    def step(self, playerPosition):\n",
        "        if (self.calculate_curr_word(self.calculate_transition(playerPosition))) != \"\":\n",
        "            self.playerState[1] += self.calculate_curr_word(self.calculate_transition(playerPosition))\n",
        "            self.reward += 1\n",
        "        else:\n",
        "            self.reward -= 0.1\n",
        "        newPlayerPosition = self.calculate_transition(playerPosition)\n",
        "        self.playerState[0] = newPlayerPosition\n",
        "        observation = self.__state  # environment is fully observable\n",
        "        done = self.get_killed_or_live()\n",
        "        if done:\n",
        "            self.reset()\n",
        "        return observation, done, self.reward, self.playerState[1]\n",
        "\n",
        "    def get_reward(self):\n",
        "        return self.reward;\n",
        "\n",
        "    def render(self):\n",
        "        BACKGROUND = [\n",
        "            ' S │   │ C │',\n",
        "            '───┼───┼───┼',\n",
        "            '   │ L │ E │',\n",
        "            '───┼───┼───┼',\n",
        "        ]\n",
        "        rendering = copy(BACKGROUND)\n",
        "        for n, S_n in enumerate(self.__state):\n",
        "            if S_n != E:\n",
        "                row = 2 * (n // 5)\n",
        "                col = 4 * (n % 5) + 1\n",
        "                line = rendering[row]\n",
        "                rendering[row] = line[:col] + S_n + line[col + 1:]\n",
        "\n",
        "        for line in rendering:\n",
        "            print(line)\n",
        "\n",
        "    # =========================================================\n",
        "    # public functions for agent to calculate optimal policy\n",
        "    # =========================================================\n",
        "\n",
        "    def calculate_transition(self, action: Action):\n",
        "        current_location = self.playerState[0]\n",
        "        next_location = None\n",
        "        if self.playerState[0] == 0:\n",
        "            if action == Action.Right:\n",
        "                next_location = current_location + 1\n",
        "            elif action == Action.Down:\n",
        "                next_location = current_location + 3\n",
        "        elif self.playerState[0] == 1:\n",
        "            if action == Action.Right:\n",
        "                next_location = current_location + 1\n",
        "            elif action == Action.Left:\n",
        "                next_location = current_location - 1\n",
        "            elif action == Action.Down:\n",
        "                next_location = current_location + 3\n",
        "        elif self.playerState[0] == 2:\n",
        "            if action == Action.Left:\n",
        "                next_location = current_location - 1\n",
        "            elif action == Action.Down:\n",
        "                next_location = current_location + 3\n",
        "        elif self.playerState[0] == 3:\n",
        "            if action == Action.Right:\n",
        "                next_location = current_location + 1\n",
        "            elif action == Action.Up:\n",
        "                next_location = current_location - 3\n",
        "        elif self.playerState[0] == 4:\n",
        "            if action == Action.Right:\n",
        "                next_location = current_location + 1\n",
        "            elif action == Action.Up:\n",
        "                next_location = current_location - 3\n",
        "            elif action == Action.Left:\n",
        "                next_location = current_location - 1\n",
        "        elif self.playerState[0] == 5:\n",
        "            if action == Action.Up:\n",
        "                next_location = current_location - 3\n",
        "            elif action == Action.Left:\n",
        "                next_location = current_location - 1\n",
        "\n",
        "        if next_location == None:\n",
        "            return current_location\n",
        "        else:\n",
        "            return next_location\n",
        "\n",
        "    def get_step_probability(self, action: Action, new_inventory):\n",
        "        next_location = self.calculate_transition(action)\n",
        "        current_inventory = self.playerState[1]\n",
        "        if next_location == 2: # State 2 contains C\n",
        "            if new_inventory == current_inventory + \"C\":\n",
        "                    return 1\n",
        "            else:\n",
        "                    return 0\n",
        "        elif next_location == 4: # State 4 contains C\n",
        "            if new_inventory == current_inventory + \"L\":\n",
        "                    return 1\n",
        "            else:\n",
        "                    return 0\n",
        "        elif current_inventory == new_inventory:\n",
        "            return 1        \n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def get_killed_or_live(self):\n",
        "        # Reward R(s) for every possible state\n",
        "        # Current word must be stored somewhere else\n",
        "        # B, BU, BUR, BURG\n",
        "        if self.playerState[1] == \"CL\" and self.playerState[0] == 5:\n",
        "            return True\n",
        "        if self.playerState[1] != \"C\" or \"CL\":\n",
        "            return False\n",
        "        return True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining the world\n",
        "After we have defined the environment, we are going to construct the world the actor will be acting in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_environment() -> RestaurantEnvironment:\n",
        "    environment = RestaurantEnvironment()\n",
        "    return environment\n",
        "\n",
        "restEnviroment = generate_environment()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " START │ C │ C │L\n",
            "───┼───┼───┼\n",
            " END │ L │ E │\n",
            "───┼───┼───┼\n"
          ]
        }
      ],
      "source": [
        "class Game():\n",
        "    # example of creation of an environment in the default state\n",
        "    mdp = RestaurantEnvironment()\n",
        "    mdp.reset()\n",
        "    mdp.render()\n",
        "    state = \"\"\n",
        "    reward = 0.0\n",
        "    done = False\n",
        "    playerPosition = 0\n",
        "    inventory = \"\"\n",
        "    \n",
        "    # state, done, reward = mdp.step(Action.Right)\n",
        "    # print(state, done, reward)\n",
        "    # state, done, reward = mdp.step(Action.Right)\n",
        "    # print(state, done, reward)\n",
        "    # state, done, reward = mdp.step(Action.Left)\n",
        "    # print(state, done, reward)\n",
        "    # state, done, reward = mdp.step(Action.Down)\n",
        "    # print(state, done, reward)\n",
        "    # state, done, reward = mdp.step(Action.Right)\n",
        "    # print(state, done, reward)\n",
        "    # print(mdp.playerState[0], mdp.playerState[1])\n",
        "    # print('possible (internal) game states:')\n",
        "\n",
        "\n",
        "game = Game()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Measuring performance\n",
        "In order to get an accurate idea of the performance of a function we define a set of helper functions which will run number of episodes with the given policy, and print some statistics such as the `mean` of the running time as well as the `standard deviation`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "from statistics import mean, stdev\n",
        "\n",
        "def run_one_episode(policy, environment, max_iteration_timeout=1000):\n",
        "  environment.reset()\n",
        "\n",
        "  state = environment\n",
        "  total_reward = 0.0\n",
        "  done = False\n",
        "  inventory = \"\"\n",
        "  \n",
        "  iteration = 0\n",
        "  while not done or iteration >= max_iteration_timeout:\n",
        "    next_action = policy(state)\n",
        "    state, done, reward, inventory = environment.step(next_action)\n",
        "    total_reward += reward\n",
        "    iteration += 1\n",
        "  return total_reward\n",
        "\n",
        "def measure_performance(policy, environment, nrof_episodes=100):\n",
        "  N = nrof_episodes\n",
        "  print(\"statistics over {} episodes\".format(N))\n",
        "  all_rewards = []\n",
        "  for _ in range(N):\n",
        "    episode_reward = run_one_episode(policy, environment)\n",
        "    all_rewards.append(episode_reward)\n",
        "  print(\"mean: {:6.2f}, sigma: {:6.2f}\".format(mean(all_rewards), stdev(all_rewards)))\n",
        "  print()\n",
        "  for n, episode_reward in enumerate(all_rewards[:5], 1):\n",
        "    print(\"ep: {:2d}, total reward: {:5.2f}\".format(n, episode_reward))\n",
        "  print(\".....\")\n",
        "  for n, episode_reward in enumerate(all_rewards[-5:], len(all_rewards) - 5):\n",
        "    print(\"ep: {:2d}, total reward: {:5.2f}\".format(n, episode_reward))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Random agent\n",
        "Here we test the performance of an agent who's policy is to pick a random action at each state. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "statistics over 100 episodes\n",
            "mean: -167.27, sigma: 334.24\n",
            "\n",
            "ep:  1, total reward:  6.60\n",
            "ep:  2, total reward: -0.10\n",
            "ep:  3, total reward: -1.40\n",
            "ep:  4, total reward: -42.00\n",
            "ep:  5, total reward: -6.10\n",
            ".....\n",
            "ep: 95, total reward: -57.30\n",
            "ep: 96, total reward: -57.40\n",
            "ep: 97, total reward: -57.50\n",
            "ep: 98, total reward: -57.60\n",
            "ep: 99, total reward: -115.50\n"
          ]
        }
      ],
      "source": [
        "def policy_random(state: RestaurantEnvironment) -> Action:\n",
        "  action = random.choice([a for a in Action])\n",
        "  return action\n",
        "\n",
        "measure_performance(policy_random, restEnviroment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### VALUE ITERATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def next_state(state: RestaurantEnvironment, action: Action):\n",
        "    done, reward, inventory = state.step(action);\n",
        "    return RestaurantEnvironment()\n",
        "\n",
        "def Q_value(environment, state, action, U):\n",
        "  new_state = next_state(environment, action)\n",
        "  reward = environment.get_reward()\n",
        "  return reward + U[str(new_state)]\n",
        "\n",
        "def Value_Iteration(environment, error=0.00001):\n",
        "    U = {}\n",
        "    U_p = get_initial_U(environment)\n",
        "    delta = float(\"inf\")\n",
        "    possible_states = environment.get_possible_states()\n",
        "\n",
        "    while delta > error:\n",
        "      for s in possible_states:\n",
        "        U[str(s)] = U_p[str(s)]\n",
        "\n",
        "      print_U(U)\n",
        "\n",
        "      delta = 0\n",
        "      for s in possible_states:\n",
        "        if s.is_done():\n",
        "          continue\n",
        "        \n",
        "        max_a = float(\"-inf\")\n",
        "\n",
        "        for a in Action:\n",
        "          q = Q_value(environment, s, a, U)\n",
        "          if q > max_a:\n",
        "            max_a = q\n",
        "        \n",
        "        U_p[str(s)] = max_a\n",
        "        difference = abs(U_p[str(s)] - U[str(s)])\n",
        "        if difference > delta:\n",
        "          delta = difference\n",
        "    \n",
        "    return U\n",
        "\n",
        "def policy_generation(environment, U):\n",
        "  pi_star = {}\n",
        "\n",
        "  for s in environment.get_possible_states():\n",
        "    if s.is_done():\n",
        "      continue\n",
        "  \n",
        "    max_a = float(\"-inf\")\n",
        "    argmax_a = None\n",
        "    for action in Action:\n",
        "      q = Q_value(environment, s, action, U)\n",
        "      if q > max_a:\n",
        "        max_a = q\n",
        "        argmax_a = action\n",
        "      pi_star[str(s)] = argmax_a\n",
        "  \n",
        "  return pi_star\n",
        "\n",
        "def get_initial_U(environment):\n",
        "  U = {}\n",
        "  for s in environment.get_possible_states():\n",
        "    U[str(s)] = 0.0\n",
        "  \n",
        "  return U\n",
        "\n",
        "def print_U(U):\n",
        "  print(U) #expand on this\n",
        "\n",
        "def print_policy(pi):\n",
        "  print(pi) #expand on this\n",
        "\n",
        "environment.reset()\n",
        "\n",
        "U = Value_Iteration(environment)\n",
        "\n",
        "print(\"final U:\")\n",
        "print_U(U)\n",
        "\n",
        "pi_star = policy_generation(environment, U)\n",
        "\n",
        "print(\"generated policy:\")\n",
        "print_policy(pi_star)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "BurgerRestauraunt.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "8c3d4d9970fc5c09c50d9318a9c7fbf4d0159e4b28833276f5678ee22bded273"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
