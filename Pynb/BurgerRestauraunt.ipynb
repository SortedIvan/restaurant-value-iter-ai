{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3> Training an AI agent to traverse an environment and collect materials in order using value iteration <h3>\n",
        "<h5> By Ivan Ovcharov & Veronika Valeva <h5>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Table of Contents\n",
        "\n",
        "* Introduction\n",
        "* Why value iteration?\n",
        "* Environment description\n",
        "* Python environment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Over the period of 9 weeks, we have been tasked to train an agent to learn over given constraints in a python environment. The project we chose to tackle is something that closely resembles the infamous <strong> frozen lake </strong> environment. With every environment, there are different ways of approaching how an agent's rules may be defined or what strategy may be used for it to <strong> \"learn\" </strong>.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After delving a bit deeper into what <strong> reinforcement learning </strong> really is, we made the decision that <strong> <i> Value Iteration </i> </strong> would be best suited for our environment and the given conditions/rules we have defined. But why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why value iteration?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For any given state, we first calculate the state-action values for all the possible <strong>actions</strong> from that given state. We then update the value function of that state with the greatest state-action value. The reason we decided not to utilize <i>policy iteration</i> instead, as we thought unnecessary to have calculations of the expected/mean state-action value. For an environment like ours, where no \"predictions\" must be made, value iteration was the best option at hand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With value iteration, we'd be able to terminate when the difference between all the new state values and the old state values is a relatively small value. Furthermore, for a grid-like environment, where all of the possible actions and <strong>\"reward\"</strong> positions are pre-defined, we'd be better off with iterating over all possible states."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ![Value iteration algorithm](images/value_iteration.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Environment description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start with defining what an <strong>environment</strong> really stands for. An environment in AI is what is surrounding the agent. The agent can take input from the environment and deliver output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As mentioned previously, the environment we have chosen to define is something that very closely resembles the <strong> frozen lake </strong> one by OpenGymAI. The rules are as follows:\n",
        "* The environment is a grid (initially a 5x5 but scaled down to a 2x2/2x3) where there are \"ingredients\" that the agent must collect.\n",
        "* The agent is only able to move up, down, left or right, depending on his position on the grid.\n",
        "* The agent may not go (for example) left, if left is outside of the grid bounds\n",
        "* The agent starts in a <i> starting state </i> and finishes in an <i> ending state</i>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once the agent collects all of the ingredients (must be done in the correct order, otherwise => agent restarts), the agent must \"leave\" by going to the end state. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The reward system that is utilized is based on getting items in correct order and finishing the game. There are also slight penalties: when agent steps on an empty cell, he loses a total of 0.2 points and when he steps on a cell, containing an ingredient => +1 points. The reason behind this is that the agent can not only learn how to collect the ingredients in the right order, but the -0.2 points serves as a bound that \"pushes\" the agent to finish the game in less moves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ![Value iteration algorithm](images/env.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Python environment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first start by defnining what kind of states there will be in our environment. In here, we define the empty state, E. Furthermore, we have one representing our imaginary lettuce and cheese, L and C respectively. Lastly, a state indicating the start and end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from enum import Enum\n",
        "from random import randint, choice\n",
        "from copy import copy\n",
        "\n",
        "E, L, C, START, END = ' ', 'L', 'C','START','END'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then define the rule set of our environment. This is done within a python class that holds all of the methods needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RestaurantEnvironment():\n",
        "    def __init__(self, initial_state=None):\n",
        "        if initial_state is None:\n",
        "            self.__initial_state = [E for n in range(6)]\n",
        "            self.__initial_state[0] = START\n",
        "            self.__initial_state[2] = C \n",
        "            self.__initial_state[4] = L\n",
        "            self.__initial_state[5] = END\n",
        "            self.__possible_states = []\n",
        "            self.playerState = [0, \"\"]\n",
        "            self.reward = 0\n",
        "        else:\n",
        "            self.__initial_state = copy(initial_state)\n",
        "            self.__state = self.__initial_state\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        self.__state = self.__initial_state\n",
        "        return self.__state\n",
        "\n",
        "    # Based on playerposition (0 to 24), add a letter to the currentWord\n",
        "    def calculate_curr_word(self, playerPosition):\n",
        "        if playerPosition == 2:\n",
        "            return \"C\"\n",
        "        elif playerPosition == 4:\n",
        "            return \"L\"\n",
        "\n",
        "    def step(self, playerPosition):\n",
        "        if (self.calculate_curr_word(playerPosition)) is not None:\n",
        "            self.playerState[1] += self.calculate_curr_word(playerPosition)\n",
        "            self.reward += 1\n",
        "        else:\n",
        "            self.reward -= 0.1\n",
        "        self.playerState[0] = playerPosition\n",
        "        observation = self.__state  # environment is fully observable\n",
        "        done = self.get_killed_or_live()\n",
        "        info = {}  # optional debug info\n",
        "        return observation, done, info\n",
        "\n",
        "    def render(self):\n",
        "        BACKGROUND = [\n",
        "            ' S │   │ C │',\n",
        "            '───┼───┼───┼',\n",
        "            '   │ L │ E │',\n",
        "            '───┼───┼───┼',\n",
        "        ]\n",
        "        rendering = copy(BACKGROUND)\n",
        "        for n, S_n in enumerate(self.__state):\n",
        "            if S_n != E:\n",
        "                row = 2 * (n // 5)\n",
        "                col = 4 * (n % 5) + 1\n",
        "                line = rendering[row]\n",
        "                rendering[row] = line[:col] + S_n + line[col + 1:]\n",
        "\n",
        "        for line in rendering:\n",
        "            print(line)\n",
        "\n",
        "    # =========================================================\n",
        "    # public functions for agent to calculate optimal policy\n",
        "    # =========================================================\n",
        "\n",
        "    def get_possible_actions(self):\n",
        "        if self.playerState[0] == 0:\n",
        "            return [self.playerState[0] + 1, self.playerState[0] + 3]\n",
        "        elif self.playerState[0] == 1:\n",
        "            return [self.playerState[0] - 1, self.playerState[0] + 1, self.playerState[0] + 3]\n",
        "        elif self.playerState[0] == 2:\n",
        "            return [self.playerState[0] -1, self.playerState[0] + 3]\n",
        "        elif self.playerState[0] == 3:\n",
        "            return [self.playerState[0] - 3, self.playerState[0] + 1]\n",
        "        elif self.playerState[0] == 4:\n",
        "            return [self.playerState[0] - 1, self.playerState[0] - 3, self.playerState[0] + 1]\n",
        "        elif self.playerState[0] == 5:\n",
        "            return [self.playerState[0] - 3, self.playerState[0] - 1]\n",
        "\n",
        "    def get_step_probability(self, new_state, new_inventory):\n",
        "        new_states = self.get_possible_actions()\n",
        "        current_inventory = self.playerState[1]\n",
        "        if new_states.__contains__(new_state):\n",
        "            if new_state == 2: # State 2 contains C\n",
        "                if new_inventory == current_inventory + \"C\":\n",
        "                    return 1\n",
        "                else:\n",
        "                    return 0\n",
        "            elif new_state == 4: # State 4 contains C\n",
        "                if new_inventory == current_inventory + \"L\":\n",
        "                    return 1\n",
        "                else:\n",
        "                    return 0\n",
        "            elif current_inventory == new_inventory:\n",
        "                return 1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def get_killed_or_live(self):\n",
        "        # Reward R(s) for every possible state\n",
        "        # Current word must be stored somewhere else\n",
        "        # B, BU, BUR, BURG\n",
        "        if self.playerState[1] != \"C\" or \"CL\":\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def get_transition_prob(self, action, new_state, old_state=None):\n",
        "        # returns the Transition Probability P(s'| s, a)\n",
        "        # with s = old_state, a = action and s' = new_state\n",
        "\n",
        "        # # if the game is over, no transition can take place\n",
        "        # if self.is_done(old_state):\n",
        "        #     return 0.0\n",
        "        #\n",
        "        # # the position of the action must be empty\n",
        "        if self.get_killed_or_live():\n",
        "            self.reset()\n",
        "            return 0.0\n",
        "\n",
        "        # check if game is done\n",
        "        if not self.get_killed_or_live():\n",
        "            self.reset()\n",
        "            return 1.0\n",
        "\n",
        "        # game is not done: calculate all possible states of the opponent\n",
        "        possible_new_states = []\n",
        "        possible_new_states = self.get_possible_actions()\n",
        "        # for action in possible_new_states:\n",
        "        #     possible_new_state = copy(state_after_X)\n",
        "        #     possible_new_state[action] = O\n",
        "        #     possible_new_states.append(possible_new_state)\n",
        "        if new_state not in possible_new_states:\n",
        "            return 0.0\n",
        "\n",
        "        # transition is possible, apply strategy:\n",
        "        # random opponent, probability is 1 / (# of E before placing the new O)\n",
        "        prob = 1 / (len(possible_new_states))\n",
        "        return prob\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " START │ C │ C │L\n",
            "───┼───┼───┼\n",
            " END │ L │ E │\n",
            "───┼───┼───┼\n",
            "(['START', ' ', 'C', ' ', 'L', 'END'], False, {})\n",
            "(['START', ' ', 'C', ' ', 'L', 'END'], False, {})\n",
            "(['START', ' ', 'C', ' ', 'L', 'END'], False, {})\n",
            "3 C\n",
            "1\n",
            "0\n",
            "possible (internal) game states:\n"
          ]
        }
      ],
      "source": [
        "class Game():\n",
        "    # example of creation of an environment in the default state\n",
        "    mdp = RestaurantEnvironment()\n",
        "    mdp.reset()\n",
        "    mdp.render()\n",
        "    print(mdp.step(1))\n",
        "    print(mdp.step(2))\n",
        "    print(mdp.step(3))\n",
        "    print(mdp.playerState[0], mdp.playerState[1])\n",
        "    print(mdp.get_step_probability(4,\"CL\"))\n",
        "    print(mdp.get_step_probability(1, \"C\"))\n",
        "    print('possible (internal) game states:')\n",
        "\n",
        "\n",
        "game = Game()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "BurgerRestauraunt.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "1c1736cc465fff578baf12fdb47ac8eb299976fae09570724a499b71de6bfef0"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
