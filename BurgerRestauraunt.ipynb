{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3> Training an AI agent to traverse an environment and collect materials in order using value iteration <h3>\n",
        "<h5> By Ivan Ovcharov & Veronika Valeva <h5>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Table of Contents\n",
        "\n",
        "* Introduction\n",
        "* Why value iteration?\n",
        "* Environment description\n",
        "* Python environment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Over the period of 9 weeks, we have been tasked to train an agent to learn over given constraints in a python environment. The project we chose to tackle is something that closely resembles the infamous <strong> frozen lake </strong> environment. With every environment, there are different ways of approaching how an agent's rules may be defined or what strategy may be used for it to <strong> \"learn\" </strong>.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After delving a bit deeper into what <strong> reinforcement learning </strong> really is, we made the decision that <strong> <i> Value Iteration </i> </strong> would be best suited for our environment and the given conditions/rules we have defined. But why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why value iteration?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For any given state, we first calculate the state-action values for all the possible <strong>actions</strong> from that given state. We then update the value function of that state with the greatest state-action value. The reason we decided not to utilize <i>policy iteration</i> instead, as we thought unnecessary to have calculations of the expected/mean state-action value. For an environment like ours, where no \"predictions\" must be made, value iteration was the best option at hand."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "BurgerRestauraunt.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "1c1736cc465fff578baf12fdb47ac8eb299976fae09570724a499b71de6bfef0"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
